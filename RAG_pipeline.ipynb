{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "iBPwos1S_hIb"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit\n",
        "!pip install faiss-gpu\n",
        "!pip install flask_ngrok\n",
        "!pip install pyngrok streamlit\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your Streamlit app to a file\n",
        "streamlit_code = \"\"\"\n",
        "import streamlit as st\n",
        "import json\n",
        "import faiss\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    DPRContextEncoder, DPRContextEncoderTokenizer,\n",
        "    DPRQuestionEncoder, DPRQuestionEncoderTokenizer,\n",
        "    T5ForConditionalGeneration, T5Tokenizer\n",
        ")\n",
        "\n",
        "@st.cache_data\n",
        "def load_corpus():\n",
        "    with open(\"/content/corpus.json\", \"r\") as file:\n",
        "        return json.load(file)\n",
        "\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "    context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "\n",
        "    question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "    question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "\n",
        "    t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "    t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "    return context_encoder, context_tokenizer, question_encoder, question_tokenizer, t5_model, t5_tokenizer\n",
        "\n",
        "@st.cache_resource\n",
        "def build_faiss_index(_corpus, _context_encoder, _context_tokenizer):\n",
        "    encoded_corpus = []\n",
        "    texts = [doc.get(\"body\", \"\") for doc in _corpus if doc.get(\"body\")]\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), 16)):  # Batch processing\n",
        "        batch_texts = texts[i:i + 16]\n",
        "        inputs = _context_tokenizer(batch_texts, return_tensors=\"pt\", max_length=256, padding=True, truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = _context_encoder(**inputs)\n",
        "        encoded_corpus.append(outputs.pooler_output.detach().numpy())\n",
        "\n",
        "    if encoded_corpus:\n",
        "        encoded_corpus = np.vstack(encoded_corpus)\n",
        "        index = faiss.IndexFlatIP(768)\n",
        "        faiss.normalize_L2(encoded_corpus)  # Normalize the vectors\n",
        "        index.add(encoded_corpus)\n",
        "        return index\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def retrieve_documents(query, question_encoder, question_tokenizer, index, corpus):\n",
        "    inputs = question_tokenizer(query, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    question_embedding = question_encoder(**inputs).pooler_output.detach().cpu().numpy()\n",
        "    D, I = index.search(question_embedding, k=3)\n",
        "    return [corpus[i] for i in I[0]]\n",
        "\n",
        "def generate_answer(query, retrieved_docs, t5_model, t5_tokenizer):\n",
        "    context = \" \".join([doc.get(\"body\", \"\") for doc in retrieved_docs])\n",
        "    input_text = f\"question: {query} context: {context}\"\n",
        "    inputs = t5_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = t5_model.generate(inputs[\"input_ids\"], max_length=150)\n",
        "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def format_output(query, answer, evidence_docs):\n",
        "    evidence_list = [\n",
        "        {\n",
        "            \"title\": doc[\"title\"],\n",
        "            \"author\": doc[\"author\"],\n",
        "            \"url\": doc.get(\"url\", \"N/A\"),\n",
        "            \"source\": doc.get(\"source\", \"N/A\"),\n",
        "            \"category\": doc.get(\"category\", \"N/A\"),\n",
        "            \"published_at\": doc.get(\"published_at\", \"N/A\"),\n",
        "            \"fact\": doc[\"body\"][:200]\n",
        "        }\n",
        "        for doc in evidence_docs\n",
        "    ]\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"question_type\": \"inference_query\",\n",
        "        \"evidence_list\": evidence_list\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    st.title(\"Document Retrieval and QA System\")\n",
        "    st.write(\"This app retrieves relevant documents from a corpus and generates an answer to your query using a T5-based model.\")\n",
        "\n",
        "    corpus = load_corpus()\n",
        "    context_encoder, context_tokenizer, question_encoder, question_tokenizer, t5_model, t5_tokenizer = load_models()\n",
        "    index = build_faiss_index(corpus, context_encoder, context_tokenizer)\n",
        "\n",
        "    query = st.text_input(\"Enter your query:\")\n",
        "    if st.button(\"Retrieve Documents and Generate Answer\"):\n",
        "        if query:\n",
        "            retrieved_docs = retrieve_documents(query, question_encoder, question_tokenizer, index, corpus)\n",
        "            answer = generate_answer(query, retrieved_docs, t5_model, t5_tokenizer)\n",
        "            output = format_output(query, answer, retrieved_docs)\n",
        "            st.write(\"### Answer:\")\n",
        "            st.write(answer)\n",
        "            st.write(\"### Supporting Documents:\")\n",
        "            for doc in output[\"evidence_list\"]:\n",
        "                st.write(f\"**Title**: {doc['title']}\")\n",
        "                st.write(f\"**Author**: {doc['author']}\")\n",
        "                st.write(f\"**Source**: {doc['source']}\")\n",
        "                st.write(f\"**Published At**: {doc['published_at']}\")\n",
        "                st.write(f\"**Fact**: {doc['fact']}\")\n",
        "                st.write(\"---\")\n",
        "        else:\n",
        "            st.warning(\"Please enter a query.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "# Save the code to a file named app.py\n",
        "with open(\"app.py\", \"w\") as file:\n",
        "    file.write(streamlit_code)\n"
      ],
      "metadata": {
        "id": "Bpd0mETZ-Fme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2nNyRpXyFuPx9l03O2RIT1pnDQt_3Gb9YL2XtEJa6ibMEjbZ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNKVMXXt96Oz",
        "outputId": "143fc696-2c5b-4f80-ec64-ad2e52dc21cf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate any running ngrok processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Run the Streamlit app in the background\n",
        "!streamlit run app.py &>/dev/null&\n",
        "\n",
        "# Create a public URL using ngrok by passing the port number directly\n",
        "public_url = ngrok.connect(8501)  # No need for named parameter \"port\"\n",
        "print(\"Public URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmUMb9l__syH",
        "outputId": "157a546f-acfa-4535-e060-052de7cb3fe7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://1767-34-16-131-15.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}