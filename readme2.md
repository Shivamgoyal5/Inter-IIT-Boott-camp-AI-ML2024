Retrieval-Augmented Generation (RAG) Pipeline

This project implements a Retrieval-Augmented Generation (RAG) pipeline aimed at answering user queries based on a large corpus of articles. The pipeline requires evidence from multiple sources and is designed to generate answers that integrate information from up to 2 to 4 documents.

Project Structure

Corpus.json: A collection of articles scraped from the web, containing the following fields for each article:
title: Title of the article.
author: Author of the article.
source: Source/publication of the article.
published_at: Publication date of the article.
category: General topic area (e.g., technology, business).
url: URL of the article.
article_body: Full text of the article.
Train.json: A dataset consisting of 2556 queries, each with evidence extracted from 2 to 4 articles. Each query contains:
query: The user's question.
answer: The answer inferred from the evidence.
question_type: In this case, it will always be "inference_query".
evidence_list: A list of documents providing supporting information for the answer. Each evidence contains:
title: Title of the article.
author: Author of the article.
url: Article URL.
source: Source/publication.
category: General topic.
published_at: Date and time of publication.
fact: A specific fact or piece of information extracted from the article.
app.py: This file contains the main pipeline code for:
Reading the Corpus.json file.
Processing the Train.json dataset.
Implementing the RAG model to answer user queries using the corpus.
Serving the model as a REST API or through a simple command-line interface (CLI).
RAG_pipeline.ipynb: A Google Colab notebook with an implementation of the pipeline using open-source models and APIs. It provides an interactive environment to run the RAG model, process queries, and generate answers using evidence from the corpus.
video.mp4: A demonstration video explaining the pipeline architecture, usage, and output. It walks through the process of answering a query using the pipeline.
Setup Instructions

1. Prerequisites
Python 3.8+
pytorch
transformers (Hugging Face)
faiss-cpu (for fast retrieval)
flask (for serving the API)
pandas and json libraries for handling the corpus and train data
2. Installation
Clone this repository:
bash
Copy code
git clone https://github.com/username/RAG-pipeline.git
cd RAG-pipeline
Install the necessary packages:
bash
Copy code
pip install -r requirements.txt
Download Corpus.json and Train.json and place them in the project root.
3. Usage
Running the pipeline via ngrok live link

Run the code in collab file it will give a live link
python app.py --query "Your query here"
The system will retrieve relevant documents from Corpus.json, process them, and return an answer along with supporting evidence.


Running in Google Colab

Open the RAG_pipeline.ipynb notebook in Google Colab and follow the instructions inside. This will allow you to interact with the model in an interactive environment.
4. File Descriptions
app.py: Contains the code for querying the RAG pipeline and serving it as a REST API.
RAG_pipeline.ipynb: Google Colab notebook for interactive use of the pipeline.
Corpus.json: Web-scraped corpus containing articles with metadata.
Train.json: Training dataset with queries and evidence used to fine-tune the pipeline.
video.mp4: A video walkthrough of the project.
5. Output Format
The output generated by the pipeline will have the following structure, similar to the entries in Train.json:

json
Copy code
{
  "query": "What is the capital of France?",
  "answer": "The capital of France is Paris.",
  "question_type": "inference_query",
  "evidence_list": [
    {
      "title": "Geography of France",
      "author": "John Doe",
      "url": "https://example.com/france-geography",
      "source": "Example Source",
      "category": "Geography",
      "published_at": "2022-01-01",
      "fact": "Paris is the capital and most populous city of France."
    }
  ]
}
6. Video Demo
Watch the video.mp4 file to see a step-by-step explanation and demo of the RAG pipeline in action.

7. Future Work
Enhancing the model by incorporating more sophisticated retrieval techniques.
Integrating additional open-source web APIs for real-time search and retrieval.
Adding more metadata analysis for more complex queries.
Let me know if you need more adjustments or specific instructions on any part!

